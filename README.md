# üèõÔ∏è Roman AI ‚Äî OMEGA CORE MAX (20/10)

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Model: Gemma3-12B](https://img.shields.io/badge/Base_Model-Gemma3--12B-blue)](https://ollama.com)
[![Logic Density: 600B+](https://img.shields.io/badge/Effective_Parameters-600B%2B-red)](https://github.com/DanielHarding/RomanAILabs)
romanailabs@gmail.com
> **"Pure Systems Engineering + Extreme Compute Amplification."**

Force a small model (12B) to behave like a 600B+ class system by massively multiplying independent reasoning mass through a hyper-layered parallel lattice. *PROTOTYPE*

---

## üöÄ The Philosophy
Small models often possess the "DNA" of correct answers but lack the "intellectual torque" to verify complex chains of thought. **OMEGA CORE MAX** bridges this gap by trading **time for intelligence**. 

On legacy hardware (e.g., Intel i5-6400), this engine delivers frontier-class accuracy at 1 token per second by unrolling 20+ specialized cognitive roles.

## üß† Key Maximizations

* **20+ Parallel Thought Roles**: Deploys a vast lattice including "Error Hunters," "Skeptics," "Math Verifiers," and "Reverse Engineers".
* **Ensemble Sampling**: Generates 3+ samples per Verifier role with temperature jittering to eliminate stochastic hallucinations.
* **Weighted Consensus Fusion**: Synthesizes final answers using entropy-weighted voting, giving a 2x multiplier to verified logic streams.
* **Multi-Strata Memory**: Implements short, mid, and long-term memory strata with AI-assisted auto-summarization and query-relevant retrieval.
* **Information Theory Metrics**: Real-time tracking of Entropy, KL-Divergence, Perplexity, and Variance to classify system stability.

## ‚öôÔ∏è Hardware Benchmarks: "The Old Beast"
This module is optimized for high-density reasoning on consumer hardware:
* **CPU**: Intel i5-6400 @ 2.70GHz.
* **RAM**: 15.5 GiB.
* **OS**: Linux Mint 22.2 Cinnamon.
* **Performance**: ~1 token/sec (Effective reasoning depth equivalent to a cluster-grade 600B model).

## üõ†Ô∏è Installation & Usage

1.  Ensure [Ollama](https://ollama.com) is running locally with `gemma3:12b`.
2.  Set your environment variable: `export OLLAMA_MODEL="gemma3:12b"`.
3.  Extract the zip
4.  Run the engine:
    ```bash
    python3 romanai-ollama-v5.py
    ```
5.  Use `/debug` in the CLI to view real-time entropy and confidence forensics.

## ‚öñÔ∏è License
Distributed under the **MIT License**. See `LICENSE` for more information.

---
**Developed by Daniel Harding ‚Äî RomanAILabs**
